EXERCICE 5 - Test case 0

================================================================================
INFORMATIONS SUR LES DONN├ëES
================================================================================
Train: 104 exemples
Test: 104 exemples
Ensemble complet L: 208 exemples
Nombre de dimensions N: 60
Nombre de poids (N+1): 61
================================================================================

Note: Utilisation de l'algorithme perceptron ONLINE (justification bas├®e sur TP1)
     - Plus efficace en m├®moire pour de grands ensembles
     - Mise ├á jour imm├®diate des poids apr├¿s chaque exemple
     - Convergence g├®n├®ralement plus rapide pour ce type de probl├¿me

[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.
[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.
[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.

============================================================
Poids du perceptron
============================================================
Biais (w[0]): -38.599297

Poids des features (w[1] ├á w[60]):
  w[1] = 72.111134
  w[2] = -10.334985
  w[3] = -106.393132
  w[4] = 41.133985
  w[5] = -12.743161
  w[6] = 30.025191
  w[7] = -39.369123
  w[8] = -31.264640
  w[9] = 54.390010
  w[10] = -31.918659
  w[11] = 25.224993
  w[12] = 44.415485
  w[13] = -21.212562
  w[14] = 1.722186
  w[15] = 21.278051
  w[16] = -29.992890
  w[17] = -28.958774
  w[18] = 37.211374
  w[19] = -26.478006
  w[20] = 62.050949
  w[21] = -67.923653
  w[22] = 78.100657
  w[23] = -62.108462
  w[24] = 60.565875
  w[25] = -25.474705
  w[26] = -10.679956
  w[27] = 27.056500
  w[28] = -11.804179
  w[29] = -13.581515
  w[30] = 66.186306
  w[31] = -86.149117
  w[32] = 35.921844
  w[33] = 15.414060
  w[34] = -26.191385
  w[35] = 14.720177
  w[36] = -0.971993
  w[37] = -32.246911
  w[38] = 3.810287
  w[39] = 35.811179
  w[40] = -45.715516
  w[41] = 12.512127
  w[42] = 6.371937
  w[43] = 9.416496
  w[44] = 17.800486
  w[45] = -18.879641
  w[46] = 34.232990
  w[47] = 2.226618
  w[48] = 74.160096
  w[49] = 99.553331
  w[50] = -488.525528
  w[51] = 167.244055
  w[52] = 358.393060
  w[53] = 139.370564
  w[54] = 8.592310
  w[55] = 79.923084
  w[56] = -125.682312
  w[57] = -153.274844
  w[58] = 102.997848
  w[59] = 106.615051
  w[60] = 99.465320
============================================================

-------------------------------------------------- 

Erreur ├á l'it├®ration 199999 : 23/208
-------------------------------------------------- 

EXERCICE 5 - Test case 1

================================================================================
INFORMATIONS SUR LES DONN├ëES
================================================================================
Train: 104 exemples
Test: 104 exemples
Ensemble complet L: 208 exemples
Nombre de dimensions N: 60
Nombre de poids (N+1): 61
================================================================================

Note: Utilisation de l'algorithme perceptron ONLINE (justification bas├®e sur TP1)
     - Plus efficace en m├®moire pour de grands ensembles
     - Mise ├á jour imm├®diate des poids apr├¿s chaque exemple
     - Convergence g├®n├®ralement plus rapide pour ce type de probl├¿me

[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.
[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.
[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.

============================================================
Poids du perceptron
============================================================
Biais (w[0]): -39.013709

Poids des features (w[1] ├á w[60]):
  w[1] = 73.356202
  w[2] = -8.521208
  w[3] = -107.935859
  w[4] = 41.425672
  w[5] = -13.757496
  w[6] = 30.352878
  w[7] = -42.577191
  w[8] = -31.248391
  w[9] = 54.689931
  w[10] = -30.320570
  w[11] = 25.341008
  w[12] = 46.353652
  w[13] = -22.749300
  w[14] = 2.153053
  w[15] = 22.944362
  w[16] = -31.891851
  w[17] = -28.400302
  w[18] = 37.196638
  w[19] = -26.146707
  w[20] = 61.736045
  w[21] = -68.677812
  w[22] = 79.129260
  w[23] = -62.992318
  w[24] = 62.204212
  w[25] = -26.941423
  w[26] = -9.525909
  w[27] = 27.587512
  w[28] = -12.067965
  w[29] = -12.256469
  w[30] = 65.338146
  w[31] = -85.796574
  w[32] = 37.035266
  w[33] = 13.294652
  w[34] = -24.475423
  w[35] = 14.160605
  w[36] = -2.174651
  w[37] = -29.655567
  w[38] = 2.873817
  w[39] = 36.026628
  w[40] = -45.088620
  w[41] = 12.856646
  w[42] = 5.499400
  w[43] = 9.120892
  w[44] = 16.518930
  w[45] = -15.598161
  w[46] = 32.536745
  w[47] = 2.356447
  w[48] = 76.074987
  w[49] = 98.765600
  w[50] = -495.442709
  w[51] = 170.613396
  w[52] = 364.048615
  w[53] = 140.321998
  w[54] = 5.899883
  w[55] = 79.068463
  w[56] = -128.591350
  w[57] = -155.036970
  w[58] = 104.388041
  w[59] = 104.313032
  w[60] = 98.772114
============================================================

-------------------------------------------------- 

Erreur ├á l'it├®ration 207998 : 0/208
-------------------------------------------------- 


================================================================================
Q. 5 - Poids perceptrons sur l'ensemble entier
================================================================================
Nombre total de lignes : 61
Taille du batch : 100
Nombre de batches n├®cessaires : 1


================================================================================
BATCH 1/1 - Lignes 1 ├á 61
================================================================================

 Poid  Perceptron test case 0  Perceptron test case 1
    0              -38.599297              -39.013709
    1               72.111134               73.356202
    2              -10.334985               -8.521208
    3             -106.393132             -107.935859
    4               41.133985               41.425672
    5              -12.743161              -13.757496
    6               30.025191               30.352878
    7              -39.369123              -42.577191
    8              -31.264640              -31.248391
    9               54.390010               54.689931
   10              -31.918659              -30.320570
   11               25.224993               25.341008
   12               44.415485               46.353652
   13              -21.212562              -22.749300
   14                1.722186                2.153053
   15               21.278051               22.944362
   16              -29.992890              -31.891851
   17              -28.958774              -28.400302
   18               37.211374               37.196638
   19              -26.478006              -26.146707
   20               62.050949               61.736045
   21              -67.923653              -68.677812
   22               78.100657               79.129260
   23              -62.108462              -62.992318
   24               60.565875               62.204212
   25              -25.474705              -26.941423
   26              -10.679956               -9.525909
   27               27.056500               27.587512
   28              -11.804179              -12.067965
   29              -13.581515              -12.256469
   30               66.186306               65.338146
   31              -86.149117              -85.796574
   32               35.921844               37.035266
   33               15.414060               13.294652
   34              -26.191385              -24.475423
   35               14.720177               14.160605
   36               -0.971993               -2.174651
   37              -32.246911              -29.655567
   38                3.810287                2.873817
   39               35.811179               36.026628
   40              -45.715516              -45.088620
   41               12.512127               12.856646
   42                6.371937                5.499400
   43                9.416496                9.120892
   44               17.800486               16.518930
   45              -18.879641              -15.598161
   46               34.232990               32.536745
   47                2.226618                2.356447
   48               74.160096               76.074987
   49               99.553331               98.765600
   50             -488.525528             -495.442709
   51              167.244055              170.613396
   52              358.393060              364.048615
   53              139.370564              140.321998
   54                8.592310                5.899883
   55               79.923084               79.068463
   56             -125.682312             -128.591350
   57             -153.274844             -155.036970
   58              102.997848              104.388041
   59              106.615051              104.313032
   60               99.465320               98.772114

[OK] Batch 1 sauvegard├® avec succ├¿s dans le fichier : **Q5\Q. 5 - Poids perceptrons sur l'ensemble entier.png**

================================================================================
R├ëSUM├ë DE L'EXPORTATION
================================================================================
Total de lignes trait├®es : 61
Taille du batch : 100
Nombre de batches g├®n├®r├®s : 1
[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.
    208 stabilit├®s calcul├®es
[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.
    208 stabilit├®s calcul├®es

================================================================================
Stabilites des perceptrons entrain├®s sur l'ensemble complet
================================================================================
Nombre total de lignes : 2
Taille du batch : 100
Nombre de batches n├®cessaires : 1


================================================================================
BATCH 1/1 - Lignes 1 ├á 2
================================================================================

 Perceptron  Patron 0  Patron 1  Patron 2  Patron 3  Patron 4  Patron 5  Patron 6  Patron 7  Patron 8  Patron 9  Patron 10  Patron 11  Patron 12  Patron 13  Patron 14  Patron 15  Patron 16  Patron 17  Patron 18  Patron 19  Patron 20  Patron 21  Patron 22  Patron 23  Patron 24  Patron 25  Patron 26  Patron 27  Patron 28  Patron 29  Patron 30  Patron 31  Patron 32  Patron 33  Patron 34  Patron 35  Patron 36  Patron 37  Patron 38  Patron 39  Patron 40  Patron 41  Patron 42  Patron 43  Patron 44  Patron 45  Patron 46  Patron 47  Patron 48  Patron 49  Patron 50  Patron 51  Patron 52  Patron 53  Patron 54  Patron 55  Patron 56  Patron 57  Patron 58  Patron 59  Patron 60  Patron 61  Patron 62  Patron 63  Patron 64  Patron 65  Patron 66  Patron 67  Patron 68  Patron 69  Patron 70  Patron 71  Patron 72  Patron 73  Patron 74  Patron 75  Patron 76  Patron 77  Patron 78  Patron 79  Patron 80  Patron 81  Patron 82  Patron 83  Patron 84  Patron 85  Patron 86  Patron 87  Patron 88  Patron 89  Patron 90  Patron 91  Patron 92  Patron 93  Patron 94  Patron 95  Patron 96  Patron 97  Patron 98  Patron 99  Patron 100  Patron 101  Patron 102  Patron 103  Patron 104  Patron 105  Patron 106  Patron 107  Patron 108  Patron 109  Patron 110  Patron 111  Patron 112  Patron 113  Patron 114  Patron 115  Patron 116  Patron 117  Patron 118  Patron 119  Patron 120  Patron 121  Patron 122  Patron 123  Patron 124  Patron 125  Patron 126  Patron 127  Patron 128  Patron 129  Patron 130  Patron 131  Patron 132  Patron 133  Patron 134  Patron 135  Patron 136  Patron 137  Patron 138  Patron 139  Patron 140  Patron 141  Patron 142  Patron 143  Patron 144  Patron 145  Patron 146  Patron 147  Patron 148  Patron 149  Patron 150  Patron 151  Patron 152  Patron 153  Patron 154  Patron 155  Patron 156  Patron 157  Patron 158  Patron 159  Patron 160  Patron 161  Patron 162  Patron 163  Patron 164  Patron 165  Patron 166  Patron 167  Patron 168  Patron 169  Patron 170  Patron 171  Patron 172  Patron 173  Patron 174  Patron 175  Patron 176  Patron 177  Patron 178  Patron 179  Patron 180  Patron 181  Patron 182  Patron 183  Patron 184  Patron 185  Patron 186  Patron 187  Patron 188  Patron 189  Patron 190  Patron 191  Patron 192  Patron 193  Patron 194  Patron 195  Patron 196  Patron 197  Patron 198  Patron 199  Patron 200  Patron 201  Patron 202  Patron 203  Patron 204  Patron 205  Patron 206  Patron 207
          0  0.000348  0.000412  0.002387  0.001475  0.000069 -0.000358  0.007429  0.000174 -0.001070  0.021186   0.007492   0.023874   0.024510   0.031638   0.020908  -0.000851  -0.000654   0.018081   0.004730   0.037744   0.077325   0.026150   0.033395   0.005863   0.007293  -0.000746  -0.000179  -0.001982  -0.002582   0.015269   0.010828   0.002344  -0.002629   0.019036   0.031576    0.02406   0.029019  -0.002678  -0.002734   0.015583   0.059795   0.046997   0.030116   0.001282  -0.002726   0.027168   0.047029   0.010879   0.003544   0.017652   0.030540   0.006502   0.004943   0.004768   0.059534     0.0048   0.003802   0.002821   0.014913   0.007946   0.063233   0.028554   0.023603   0.004170   0.003374   0.003139   0.004283   0.007663   0.022352   0.009777   0.002915   0.009621   0.037377   0.003205   0.012243   0.011611   0.006820   0.003579   0.010449   0.033395   0.061921   0.003486   0.003367   0.011534   0.002639   0.013801   0.027058   0.011340   0.022024   0.068912   0.028969   0.023375   0.014311    0.03674   0.002915   0.002226   0.003899   0.001853   0.038214   0.004137    0.064384    0.012721    0.001357    0.030934    0.000813    0.000776    0.000301    0.021354    0.000585   -0.000047   -0.000371    0.028660    0.030305   -0.000397    0.029627    0.006455    0.014835    0.002635    0.012663    0.006203    -0.00028    0.007353    0.015552    0.019932   -0.000434    0.038037    0.062491    0.027980    0.030230    0.039674    0.000525    0.092408    0.016123    0.025437   -0.000436    0.007053    0.010650    0.004836   -0.000676    0.026354   -0.000351    0.008776    0.012710    0.020197    0.034423    0.027808    0.027965    0.084476    0.063843    0.049636    0.056700    -0.00018    0.019768    0.003881    0.015065   -0.001396    0.004388    0.024762    0.032711    0.039799    0.039391    0.022525   -0.000302    0.015468    0.000700    0.007883    0.028134    0.004162    0.047053    0.003663    0.034866    0.003161    0.033485    0.003268    0.031295    0.010363    0.009099    0.025824    0.021840    0.003923    0.003768    0.045173    0.004492    0.003767    0.007198    0.003968    0.016910    0.014482    0.014845    0.011261    0.050016    0.066360    0.067609    0.032352    0.021710    0.027888    0.002272    0.015985     0.04302    0.027314    0.002983    0.002519    0.036524    0.027528    0.016098    0.031853    0.001294    0.002255
          1  0.001564  0.002267  0.002413  0.002407  0.000495  0.000434  0.008069  0.001159  0.000533  0.022554   0.009622   0.025866   0.026042   0.033634   0.023359   0.000876   0.001243   0.020167   0.007353   0.039243   0.078637   0.026777   0.033397   0.007979   0.008499   0.000106   0.000290   0.001434   0.000931   0.017136   0.012087   0.004704   0.000628   0.021251   0.033433    0.02551   0.032344   0.000358   0.000192   0.018351   0.063589   0.050454   0.032135   0.004053   0.000009   0.029554   0.049096   0.013316   0.005805   0.016531   0.027963   0.002746   0.002389   0.001940   0.055334     0.0021   0.002042   0.002101   0.012449   0.006222   0.061747   0.026868   0.020709   0.002354   0.002086   0.002246   0.002163   0.005892   0.021655   0.007882   0.001677   0.007752   0.035630   0.001955   0.011984   0.011973   0.004561   0.001814   0.008720   0.031373   0.058450   0.001912   0.000859   0.010362   0.001516   0.011725   0.024525   0.008378   0.019301   0.066652   0.027194   0.021915   0.012251    0.03485   0.001692   0.000995   0.001122   0.000196   0.035500   0.002762    0.062062    0.011648    0.000037    0.028490    0.001834    0.002740    0.001512    0.022541    0.001289    0.001238    0.000971    0.029528    0.031575    0.000932    0.030955    0.008969    0.017259    0.004987    0.015442    0.008038     0.00067    0.008313    0.017769    0.022322    0.001154    0.038322    0.064684    0.031848    0.032509    0.041142    0.002587    0.092820    0.017242    0.026544    0.000509    0.007338    0.014157    0.008098    0.002980    0.030137    0.000839    0.009545    0.015391    0.022469    0.037569    0.031192    0.031075    0.086843    0.067249    0.052327    0.060017     0.00304    0.021883    0.006551    0.017559    0.000918    0.006392    0.026755    0.034814    0.042104    0.041482    0.024439    0.002666    0.017429    0.003193    0.010690    0.025219    0.002095    0.044835    0.001994    0.033648    0.002203    0.032066    0.001412    0.029124    0.008711    0.006784    0.022499    0.019381    0.002728    0.001270    0.042881    0.002961    0.001604    0.005101    0.001645    0.013983    0.012014    0.011970    0.008606    0.047646    0.063504    0.065243    0.029601    0.020333    0.026451    0.000047    0.013993     0.04040    0.026204    0.000244    0.000463    0.033894    0.025043    0.012502    0.030389    0.000421    0.000560

[OK] Batch 1 sauvegard├® avec succ├¿s dans le fichier : **Q5\Q5_Stabilites des perceptrons entrain├®s sur l'ensemble complet.png**

================================================================================
R├ëSUM├ë DE L'EXPORTATION
================================================================================
Total de lignes trait├®es : 2
Taille du batch : 100
Nombre de batches g├®n├®r├®s : 1
Iterations 200000 : Erreur = 23/208 = 11.06%
Iterations 500000 : Erreur = 0/208 = 0.00%
-------------------------------------------------- 

CONCLUSION
L'algortihme ne converge jamais : on en conclut que L n'est pas lin├®airement s├®parable. En effet, P = 208 > 2N = 120=> l'existence d'un hyperplan s├®parateur n'est pas garantie.

================================================================================
FIN DU TP2
================================================================================
