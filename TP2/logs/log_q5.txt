EXERCICE 5 - Test case 0

================================================================================
INFORMATIONS SUR LES DONN├ëES
================================================================================
Train: 104 exemples
Test: 104 exemples
Ensemble complet L: 208 exemples
Nombre de dimensions N: 60
Nombre de poids (N+1): 61
================================================================================

Note: Utilisation de l'algorithme perceptron ONLINE (justification bas├®e sur TP1)
     - Plus efficace en m├®moire pour de grands ensembles
     - Mise ├á jour imm├®diate des poids apr├¿s chaque exemple
     - Convergence g├®n├®ralement plus rapide pour ce type de probl├¿me

[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.
[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.
[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.

============================================================
Poids du perceptron
============================================================
Biais (w[0]): -12.661976

Poids des features (w[1] ├á w[60]):
  w[1] = 39.684840
  w[2] = 18.277663
  w[3] = -57.288084
  w[4] = 22.242538
  w[5] = 6.318414
  w[6] = -5.208594
  w[7] = -14.110712
  w[8] = -7.244626
  w[9] = 2.840069
  w[10] = -8.336223
  w[11] = 13.553744
  w[12] = 10.103664
  w[13] = -1.473289
  w[14] = -10.797682
  w[15] = 17.964645
  w[16] = -11.886473
  w[17] = -9.794231
  w[18] = 14.875497
  w[19] = -7.587779
  w[20] = 14.473517
  w[21] = -16.474455
  w[22] = 19.015083
  w[23] = -14.046231
  w[24] = 11.602446
  w[25] = 0.311399
  w[26] = -10.573568
  w[27] = 9.713404
  w[28] = -4.455962
  w[29] = -8.022848
  w[30] = 20.835478
  w[31] = -21.883347
  w[32] = 1.937713
  w[33] = 15.480858
  w[34] = -15.185142
  w[35] = 10.756754
  w[36] = -8.355184
  w[37] = -6.390552
  w[38] = -3.739389
  w[39] = 10.934205
  w[40] = -14.090011
  w[41] = 6.896221
  w[42] = -2.151774
  w[43] = 4.486694
  w[44] = 15.035569
  w[45] = -16.321554
  w[46] = 27.492925
  w[47] = -33.928807
  w[48] = 45.929685
  w[49] = 39.072015
  w[50] = -65.844757
  w[51] = 41.029812
  w[52] = 53.981864
  w[53] = 50.386054
  w[54] = 36.008292
  w[55] = 6.433370
  w[56] = 6.058437
  w[57] = -27.700614
  w[58] = 43.004501
  w[59] = 52.059692
  w[60] = 35.780637
============================================================

-------------------------------------------------- 

Erreur ├á l'it├®ration 9999 : 65/208
-------------------------------------------------- 

EXERCICE 5 - Test case 1

================================================================================
INFORMATIONS SUR LES DONN├ëES
================================================================================
Train: 104 exemples
Test: 104 exemples
Ensemble complet L: 208 exemples
Nombre de dimensions N: 60
Nombre de poids (N+1): 61
================================================================================

Note: Utilisation de l'algorithme perceptron ONLINE (justification bas├®e sur TP1)
     - Plus efficace en m├®moire pour de grands ensembles
     - Mise ├á jour imm├®diate des poids apr├¿s chaque exemple
     - Convergence g├®n├®ralement plus rapide pour ce type de probl├¿me

[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.
[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.
[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.

============================================================
Poids du perceptron
============================================================
Biais (w[0]): -15.891709

Poids des features (w[1] ├á w[60]):
  w[1] = 42.987105
  w[2] = 10.853763
  w[3] = -60.263074
  w[4] = 26.487161
  w[5] = 1.799221
  w[6] = 0.642582
  w[7] = -28.670807
  w[8] = -7.568546
  w[9] = 8.709926
  w[10] = -7.751247
  w[11] = 15.212608
  w[12] = 11.977847
  w[13] = -1.846782
  w[14] = -15.215643
  w[15] = 24.482245
  w[16] = -17.556800
  w[17] = -10.184692
  w[18] = 17.481032
  w[19] = -8.029649
  w[20] = 15.948608
  w[21] = -17.349543
  w[22] = 20.001710
  w[23] = -13.885301
  w[24] = 11.646261
  w[25] = 3.126468
  w[26] = -14.238531
  w[27] = 14.617048
  w[28] = -7.398585
  w[29] = -7.503012
  w[30] = 24.131533
  w[31] = -27.497497
  w[32] = 1.849806
  w[33] = 23.224445
  w[34] = -20.931299
  w[35] = 12.904970
  w[36] = -5.709691
  w[37] = -13.775014
  w[38] = -2.582020
  w[39] = 15.974020
  w[40] = -21.550880
  w[41] = 11.125052
  w[42] = -2.951352
  w[43] = 3.185487
  w[44] = 15.648470
  w[45] = -15.737246
  w[46] = 29.225152
  w[47] = -25.273531
  w[48] = 43.889458
  w[49] = 57.880970
  w[50] = -115.397664
  w[51] = 41.367511
  w[52] = 101.762210
  w[53] = 67.990538
  w[54] = 35.863104
  w[55] = 20.842367
  w[56] = 0.602069
  w[57] = -43.207954
  w[58] = 68.860408
  w[59] = 84.832894
  w[60] = 45.902421
============================================================

-------------------------------------------------- 

Erreur ├á l'it├®ration 19999 : 52/208
-------------------------------------------------- 

EXERCICE 5 - Test case 2

================================================================================
INFORMATIONS SUR LES DONN├ëES
================================================================================
Train: 104 exemples
Test: 104 exemples
Ensemble complet L: 208 exemples
Nombre de dimensions N: 60
Nombre de poids (N+1): 61
================================================================================

Note: Utilisation de l'algorithme perceptron ONLINE (justification bas├®e sur TP1)
     - Plus efficace en m├®moire pour de grands ensembles
     - Mise ├á jour imm├®diate des poids apr├¿s chaque exemple
     - Convergence g├®n├®ralement plus rapide pour ce type de probl├¿me

[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.
[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.
[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.

============================================================
Poids du perceptron
============================================================
Biais (w[0]): -17.849540

Poids des features (w[1] ├á w[60]):
  w[1] = 46.363470
  w[2] = 3.768842
  w[3] = -66.140769
  w[4] = 29.833213
  w[5] = -2.408395
  w[6] = 8.731435
  w[7] = -36.769000
  w[8] = -10.642177
  w[9] = 11.557405
  w[10] = -6.657634
  w[11] = 17.175435
  w[12] = 13.785765
  w[13] = -2.806497
  w[14] = -14.877198
  w[15] = 24.446797
  w[16] = -19.806142
  w[17] = -11.044255
  w[18] = 18.600152
  w[19] = -9.831484
  w[20] = 21.217974
  w[21] = -21.655188
  w[22] = 25.094353
  w[23] = -17.349533
  w[24] = 14.411190
  w[25] = 3.796330
  w[26] = -16.729323
  w[27] = 16.513017
  w[28] = -9.876087
  w[29] = -5.953479
  w[30] = 28.299502
  w[31] = -33.962602
  w[32] = 3.788941
  w[33] = 26.300742
  w[34] = -24.790067
  w[35] = 14.308347
  w[36] = -4.065043
  w[37] = -18.097497
  w[38] = -1.022160
  w[39] = 18.826959
  w[40] = -24.657757
  w[41] = 11.094070
  w[42] = -0.288397
  w[43] = 2.354783
  w[44] = 15.737719
  w[45] = -13.862617
  w[46] = 31.166394
  w[47] = -23.969651
  w[48] = 44.389205
  w[49] = 75.334941
  w[50] = -151.891140
  w[51] = 50.392073
  w[52] = 137.544719
  w[53] = 76.617509
  w[54] = 38.570219
  w[55] = 38.591532
  w[56] = -4.175785
  w[57] = -49.423012
  w[58] = 83.777112
  w[59] = 99.674329
  w[60] = 50.846086
============================================================

-------------------------------------------------- 

Erreur ├á l'it├®ration 29999 : 31/208
-------------------------------------------------- 

EXERCICE 5 - Test case 3

================================================================================
INFORMATIONS SUR LES DONN├ëES
================================================================================
Train: 104 exemples
Test: 104 exemples
Ensemble complet L: 208 exemples
Nombre de dimensions N: 60
Nombre de poids (N+1): 61
================================================================================

Note: Utilisation de l'algorithme perceptron ONLINE (justification bas├®e sur TP1)
     - Plus efficace en m├®moire pour de grands ensembles
     - Mise ├á jour imm├®diate des poids apr├¿s chaque exemple
     - Convergence g├®n├®ralement plus rapide pour ce type de probl├¿me

[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.
[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.
[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.

============================================================
Poids du perceptron
============================================================
Biais (w[0]): -22.499866

Poids des features (w[1] ├á w[60]):
  w[1] = 54.446882
  w[2] = -2.203738
  w[3] = -77.343260
  w[4] = 36.460418
  w[5] = -7.369578
  w[6] = 15.870602
  w[7] = -41.538187
  w[8] = -12.776052
  w[9] = 12.934479
  w[10] = -7.355490
  w[11] = 19.790549
  w[12] = 19.548561
  w[13] = -5.567248
  w[14] = -14.535711
  w[15] = 24.567203
  w[16] = -23.083757
  w[17] = -13.277379
  w[18] = 20.492867
  w[19] = -10.593110
  w[20] = 28.162123
  w[21] = -29.204159
  w[22] = 33.523349
  w[23] = -24.493404
  w[24] = 20.131502
  w[25] = 1.508018
  w[26] = -18.001032
  w[27] = 19.936375
  w[28] = -11.373083
  w[29] = -5.515880
  w[30] = 35.776794
  w[31] = -44.439283
  w[32] = 9.274005
  w[33] = 26.339197
  w[34] = -26.525313
  w[35] = 13.705562
  w[36] = -1.735973
  w[37] = -23.410077
  w[38] = 1.105231
  w[39] = 21.616792
  w[40] = -27.303478
  w[41] = 8.825125
  w[42] = 4.168075
  w[43] = 1.485289
  w[44] = 17.730670
  w[45] = -13.998015
  w[46] = 28.986227
  w[47] = -16.592546
  w[48] = 42.984370
  w[49] = 100.005808
  w[50] = -211.838547
  w[51] = 67.315168
  w[52] = 188.163947
  w[53] = 88.713414
  w[54] = 38.339179
  w[55] = 60.486355
  w[56] = -13.891045
  w[57] = -71.434777
  w[58] = 100.064262
  w[59] = 112.036701
  w[60] = 63.420910
============================================================

-------------------------------------------------- 

Erreur ├á l'it├®ration 49999 : 25/208
-------------------------------------------------- 

EXERCICE 5 - Test case 4

================================================================================
INFORMATIONS SUR LES DONN├ëES
================================================================================
Train: 104 exemples
Test: 104 exemples
Ensemble complet L: 208 exemples
Nombre de dimensions N: 60
Nombre de poids (N+1): 61
================================================================================

Note: Utilisation de l'algorithme perceptron ONLINE (justification bas├®e sur TP1)
     - Plus efficace en m├®moire pour de grands ensembles
     - Mise ├á jour imm├®diate des poids apr├¿s chaque exemple
     - Convergence g├®n├®ralement plus rapide pour ce type de probl├¿me

[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.
[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.
[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.

============================================================
Poids du perceptron
============================================================
Biais (w[0]): -33.194905

Poids des features (w[1] ├á w[60]):
  w[1] = 67.587664
  w[2] = -13.572991
  w[3] = -86.253905
  w[4] = 45.984744
  w[5] = -9.127962
  w[6] = 11.691605
  w[7] = -32.366224
  w[8] = -15.454535
  w[9] = 23.371233
  w[10] = -11.592790
  w[11] = 15.556659
  w[12] = 31.425973
  w[13] = -12.619049
  w[14] = -7.746493
  w[15] = 20.871375
  w[16] = -24.321768
  w[17] = -22.546189
  w[18] = 32.370349
  w[19] = -21.394109
  w[20] = 48.984096
  w[21] = -49.593596
  w[22] = 56.669556
  w[23] = -45.777118
  w[24] = 37.381692
  w[25] = -6.736055
  w[26] = -15.951924
  w[27] = 23.648017
  w[28] = -10.846943
  w[29] = -9.979641
  w[30] = 50.231935
  w[31] = -62.361813
  w[32] = 22.108179
  w[33] = 19.381601
  w[34] = -22.085762
  w[35] = 9.916589
  w[36] = -1.299329
  w[37] = -27.025352
  w[38] = 3.660226
  w[39] = 28.260905
  w[40] = -37.364153
  w[41] = 9.399049
  w[42] = 7.890081
  w[43] = 2.564690
  w[44] = 19.738898
  w[45] = -19.897220
  w[46] = 38.141530
  w[47] = -10.365945
  w[48] = 52.137659
  w[49] = 119.403539
  w[50] = -351.527486
  w[51] = 97.759853
  w[52] = 270.987529
  w[53] = 103.218778
  w[54] = 33.627307
  w[55] = 75.440398
  w[56] = -56.301065
  w[57] = -113.308252
  w[58] = 99.122562
  w[59] = 120.010674
  w[60] = 89.607270
============================================================

-------------------------------------------------- 

Erreur ├á l'it├®ration 99999 : 32/208
-------------------------------------------------- 

EXERCICE 5 - Test case 5

================================================================================
INFORMATIONS SUR LES DONN├ëES
================================================================================
Train: 104 exemples
Test: 104 exemples
Ensemble complet L: 208 exemples
Nombre de dimensions N: 60
Nombre de poids (N+1): 61
================================================================================

Note: Utilisation de l'algorithme perceptron ONLINE (justification bas├®e sur TP1)
     - Plus efficace en m├®moire pour de grands ensembles
     - Mise ├á jour imm├®diate des poids apr├¿s chaque exemple
     - Convergence g├®n├®ralement plus rapide pour ce type de probl├¿me

[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.
[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.
[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.

============================================================
Poids du perceptron
============================================================
Biais (w[0]): -38.682625

Poids des features (w[1] ├á w[60]):
  w[1] = 72.287212
  w[2] = -10.721286
  w[3] = -106.234326
  w[4] = 40.949881
  w[5] = -13.023859
  w[6] = 30.553020
  w[7] = -39.697913
  w[8] = -31.521735
  w[9] = 54.466273
  w[10] = -31.991529
  w[11] = 25.244053
  w[12] = 44.694933
  w[13] = -20.814268
  w[14] = 1.749828
  w[15] = 21.355895
  w[16] = -29.792983
  w[17] = -28.822894
  w[18] = 37.031503
  w[19] = -26.276616
  w[20] = 61.994985
  w[21] = -67.912979
  w[22] = 77.965548
  w[23] = -62.311825
  w[24] = 60.691821
  w[25] = -25.304903
  w[26] = -10.748387
  w[27] = 27.086554
  w[28] = -11.755504
  w[29] = -13.358558
  w[30] = 66.034536
  w[31] = -86.152291
  w[32] = 36.007671
  w[33] = 15.396122
  w[34] = -26.001524
  w[35] = 15.249841
  w[36] = -0.525318
  w[37] = -32.147930
  w[38] = 3.592348
  w[39] = 35.999505
  w[40] = -45.401925
  w[41] = 12.629593
  w[42] = 6.619539
  w[43] = 9.271696
  w[44] = 17.473456
  w[45] = -18.895558
  w[46] = 33.892505
  w[47] = 2.997101
  w[48] = 74.078769
  w[49] = 99.852261
  w[50] = -488.642426
  w[51] = 167.047751
  w[52] = 359.192652
  w[53] = 139.012759
  w[54] = 8.989842
  w[55] = 79.614457
  w[56] = -125.857082
  w[57] = -153.639646
  w[58] = 103.035654
  w[59] = 106.424449
  w[60] = 99.822082
============================================================

-------------------------------------------------- 

Erreur ├á l'it├®ration 199999 : 9/208
-------------------------------------------------- 

EXERCICE 5 - Test case 6

================================================================================
INFORMATIONS SUR LES DONN├ëES
================================================================================
Train: 104 exemples
Test: 104 exemples
Ensemble complet L: 208 exemples
Nombre de dimensions N: 60
Nombre de poids (N+1): 61
================================================================================

Note: Utilisation de l'algorithme perceptron ONLINE (justification bas├®e sur TP1)
     - Plus efficace en m├®moire pour de grands ensembles
     - Mise ├á jour imm├®diate des poids apr├¿s chaque exemple
     - Convergence g├®n├®ralement plus rapide pour ce type de probl├¿me

[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.
[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.
[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.

============================================================
Poids du perceptron
============================================================
Biais (w[0]): -38.865031

Poids des features (w[1] ├á w[60]):
  w[1] = 73.916846
  w[2] = -8.093832
  w[3] = -108.415328
  w[4] = 40.894240
  w[5] = -13.879237
  w[6] = 29.867627
  w[7] = -42.343772
  w[8] = -31.640669
  w[9] = 54.654366
  w[10] = -29.918506
  w[11] = 25.829252
  w[12] = 46.050500
  w[13] = -23.024270
  w[14] = 2.679578
  w[15] = 22.363955
  w[16] = -31.795166
  w[17] = -28.532580
  w[18] = 37.321137
  w[19] = -26.084406
  w[20] = 61.421986
  w[21] = -68.585918
  w[22] = 79.450969
  w[23] = -63.625624
  w[24] = 62.643405
  w[25] = -27.239320
  w[26] = -9.600521
  w[27] = 27.276091
  w[28] = -11.834266
  w[29] = -12.759275
  w[30] = 66.098362
  w[31] = -85.919815
  w[32] = 37.239206
  w[33] = 13.412240
  w[34] = -24.971226
  w[35] = 14.408242
  w[36] = -2.126129
  w[37] = -29.649453
  w[38] = 2.527109
  w[39] = 36.232601
  w[40] = -45.013149
  w[41] = 12.940108
  w[42] = 5.310738
  w[43] = 9.746927
  w[44] = 16.443945
  w[45] = -15.564776
  w[46] = 33.263825
  w[47] = 2.014115
  w[48] = 76.736359
  w[49] = 98.249929
  w[50] = -497.038708
  w[51] = 171.298378
  w[52] = 365.012763
  w[53] = 140.510539
  w[54] = 5.313515
  w[55] = 78.502762
  w[56] = -129.178444
  w[57] = -155.709659
  w[58] = 104.812000
  w[59] = 104.761137
  w[60] = 98.622228
============================================================

-------------------------------------------------- 

Erreur ├á l'it├®ration 209759 : 0/208
-------------------------------------------------- 


================================================================================
Q. 5 - Poids perceptrons sur l'ensemble entier
================================================================================
Nombre total de lignes : 61
Taille du batch : 100
Nombre de batches n├®cessaires : 1


================================================================================
BATCH 1/1 - Lignes 1 ├á 61
================================================================================

 Poid  Perceptron test case 0  Perceptron test case 1  Perceptron test case 2  Perceptron test case 3  Perceptron test case 4  Perceptron test case 5  Perceptron test case 6
    0              -12.661976              -15.891709              -17.849540              -22.499866              -33.194905              -38.682625              -38.865031
    1               39.684840               42.987105               46.363470               54.446882               67.587664               72.287212               73.916846
    2               18.277663               10.853763                3.768842               -2.203738              -13.572991              -10.721286               -8.093832
    3              -57.288084              -60.263074              -66.140769              -77.343260              -86.253905             -106.234326             -108.415328
    4               22.242538               26.487161               29.833213               36.460418               45.984744               40.949881               40.894240
    5                6.318414                1.799221               -2.408395               -7.369578               -9.127962              -13.023859              -13.879237
    6               -5.208594                0.642582                8.731435               15.870602               11.691605               30.553020               29.867627
    7              -14.110712              -28.670807              -36.769000              -41.538187              -32.366224              -39.697913              -42.343772
    8               -7.244626               -7.568546              -10.642177              -12.776052              -15.454535              -31.521735              -31.640669
    9                2.840069                8.709926               11.557405               12.934479               23.371233               54.466273               54.654366
   10               -8.336223               -7.751247               -6.657634               -7.355490              -11.592790              -31.991529              -29.918506
   11               13.553744               15.212608               17.175435               19.790549               15.556659               25.244053               25.829252
   12               10.103664               11.977847               13.785765               19.548561               31.425973               44.694933               46.050500
   13               -1.473289               -1.846782               -2.806497               -5.567248              -12.619049              -20.814268              -23.024270
   14              -10.797682              -15.215643              -14.877198              -14.535711               -7.746493                1.749828                2.679578
   15               17.964645               24.482245               24.446797               24.567203               20.871375               21.355895               22.363955
   16              -11.886473              -17.556800              -19.806142              -23.083757              -24.321768              -29.792983              -31.795166
   17               -9.794231              -10.184692              -11.044255              -13.277379              -22.546189              -28.822894              -28.532580
   18               14.875497               17.481032               18.600152               20.492867               32.370349               37.031503               37.321137
   19               -7.587779               -8.029649               -9.831484              -10.593110              -21.394109              -26.276616              -26.084406
   20               14.473517               15.948608               21.217974               28.162123               48.984096               61.994985               61.421986
   21              -16.474455              -17.349543              -21.655188              -29.204159              -49.593596              -67.912979              -68.585918
   22               19.015083               20.001710               25.094353               33.523349               56.669556               77.965548               79.450969
   23              -14.046231              -13.885301              -17.349533              -24.493404              -45.777118              -62.311825              -63.625624
   24               11.602446               11.646261               14.411190               20.131502               37.381692               60.691821               62.643405
   25                0.311399                3.126468                3.796330                1.508018               -6.736055              -25.304903              -27.239320
   26              -10.573568              -14.238531              -16.729323              -18.001032              -15.951924              -10.748387               -9.600521
   27                9.713404               14.617048               16.513017               19.936375               23.648017               27.086554               27.276091
   28               -4.455962               -7.398585               -9.876087              -11.373083              -10.846943              -11.755504              -11.834266
   29               -8.022848               -7.503012               -5.953479               -5.515880               -9.979641              -13.358558              -12.759275
   30               20.835478               24.131533               28.299502               35.776794               50.231935               66.034536               66.098362
   31              -21.883347              -27.497497              -33.962602              -44.439283              -62.361813              -86.152291              -85.919815
   32                1.937713                1.849806                3.788941                9.274005               22.108179               36.007671               37.239206
   33               15.480858               23.224445               26.300742               26.339197               19.381601               15.396122               13.412240
   34              -15.185142              -20.931299              -24.790067              -26.525313              -22.085762              -26.001524              -24.971226
   35               10.756754               12.904970               14.308347               13.705562                9.916589               15.249841               14.408242
   36               -8.355184               -5.709691               -4.065043               -1.735973               -1.299329               -0.525318               -2.126129
   37               -6.390552              -13.775014              -18.097497              -23.410077              -27.025352              -32.147930              -29.649453
   38               -3.739389               -2.582020               -1.022160                1.105231                3.660226                3.592348                2.527109
   39               10.934205               15.974020               18.826959               21.616792               28.260905               35.999505               36.232601
   40              -14.090011              -21.550880              -24.657757              -27.303478              -37.364153              -45.401925              -45.013149
   41                6.896221               11.125052               11.094070                8.825125                9.399049               12.629593               12.940108
   42               -2.151774               -2.951352               -0.288397                4.168075                7.890081                6.619539                5.310738
   43                4.486694                3.185487                2.354783                1.485289                2.564690                9.271696                9.746927
   44               15.035569               15.648470               15.737719               17.730670               19.738898               17.473456               16.443945
   45              -16.321554              -15.737246              -13.862617              -13.998015              -19.897220              -18.895558              -15.564776
   46               27.492925               29.225152               31.166394               28.986227               38.141530               33.892505               33.263825
   47              -33.928807              -25.273531              -23.969651              -16.592546              -10.365945                2.997101                2.014115
   48               45.929685               43.889458               44.389205               42.984370               52.137659               74.078769               76.736359
   49               39.072015               57.880970               75.334941              100.005808              119.403539               99.852261               98.249929
   50              -65.844757             -115.397664             -151.891140             -211.838547             -351.527486             -488.642426             -497.038708
   51               41.029812               41.367511               50.392073               67.315168               97.759853              167.047751              171.298378
   52               53.981864              101.762210              137.544719              188.163947              270.987529              359.192652              365.012763
   53               50.386054               67.990538               76.617509               88.713414              103.218778              139.012759              140.510539
   54               36.008292               35.863104               38.570219               38.339179               33.627307                8.989842                5.313515
   55                6.433370               20.842367               38.591532               60.486355               75.440398               79.614457               78.502762
   56                6.058437                0.602069               -4.175785              -13.891045              -56.301065             -125.857082             -129.178444
   57              -27.700614              -43.207954              -49.423012              -71.434777             -113.308252             -153.639646             -155.709659
   58               43.004501               68.860408               83.777112              100.064262               99.122562              103.035654              104.812000
   59               52.059692               84.832894               99.674329              112.036701              120.010674              106.424449              104.761137
   60               35.780637               45.902421               50.846086               63.420910               89.607270               99.822082               98.622228

[OK] Batch 1 sauvegard├® avec succ├¿s dans le fichier : **Q5\Q. 5 - Poids perceptrons sur l'ensemble entier.png**

================================================================================
R├ëSUM├ë DE L'EXPORTATION
================================================================================
Total de lignes trait├®es : 61
Taille du batch : 100
Nombre de batches g├®n├®r├®s : 1
[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.
    208 stabilit├®s calcul├®es
[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.
    208 stabilit├®s calcul├®es
[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.
    208 stabilit├®s calcul├®es
[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.
    208 stabilit├®s calcul├®es
[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.
    208 stabilit├®s calcul├®es
[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.
    208 stabilit├®s calcul├®es
[Avertissement] Conversion du DataFrame en Liste pour la fonction d'initialisation.
    208 stabilit├®s calcul├®es

================================================================================
Stabilites des perceptrons entrain├®s sur l'ensemble complet
================================================================================
Nombre total de lignes : 7
Taille du batch : 100
Nombre de batches n├®cessaires : 1


================================================================================
BATCH 1/1 - Lignes 1 ├á 7
================================================================================

 Perceptron  Patron 0  Patron 1  Patron 2  Patron 3  Patron 4  Patron 5  Patron 6  Patron 7  Patron 8  Patron 9  Patron 10  Patron 11  Patron 12  Patron 13  Patron 14  Patron 15  Patron 16  Patron 17  Patron 18  Patron 19  Patron 20  Patron 21  Patron 22  Patron 23  Patron 24  Patron 25  Patron 26  Patron 27  Patron 28  Patron 29  Patron 30  Patron 31  Patron 32  Patron 33  Patron 34  Patron 35  Patron 36  Patron 37  Patron 38  Patron 39  Patron 40  Patron 41  Patron 42  Patron 43  Patron 44  Patron 45  Patron 46  Patron 47  Patron 48  Patron 49  Patron 50  Patron 51  Patron 52  Patron 53  Patron 54  Patron 55  Patron 56  Patron 57  Patron 58  Patron 59  Patron 60  Patron 61  Patron 62  Patron 63  Patron 64  Patron 65  Patron 66  Patron 67  Patron 68  Patron 69  Patron 70  Patron 71  Patron 72  Patron 73  Patron 74  Patron 75  Patron 76  Patron 77  Patron 78  Patron 79  Patron 80  Patron 81  Patron 82  Patron 83  Patron 84  Patron 85  Patron 86  Patron 87  Patron 88  Patron 89  Patron 90  Patron 91  Patron 92  Patron 93  Patron 94  Patron 95  Patron 96  Patron 97  Patron 98  Patron 99  Patron 100  Patron 101  Patron 102  Patron 103  Patron 104  Patron 105  Patron 106  Patron 107  Patron 108  Patron 109  Patron 110  Patron 111  Patron 112  Patron 113  Patron 114  Patron 115  Patron 116  Patron 117  Patron 118  Patron 119  Patron 120  Patron 121  Patron 122  Patron 123  Patron 124  Patron 125  Patron 126  Patron 127  Patron 128  Patron 129  Patron 130  Patron 131  Patron 132  Patron 133  Patron 134  Patron 135  Patron 136  Patron 137  Patron 138  Patron 139  Patron 140  Patron 141  Patron 142  Patron 143  Patron 144  Patron 145  Patron 146  Patron 147  Patron 148  Patron 149  Patron 150  Patron 151  Patron 152  Patron 153  Patron 154  Patron 155  Patron 156  Patron 157  Patron 158  Patron 159  Patron 160  Patron 161  Patron 162  Patron 163  Patron 164  Patron 165  Patron 166  Patron 167  Patron 168  Patron 169  Patron 170  Patron 171  Patron 172  Patron 173  Patron 174  Patron 175  Patron 176  Patron 177  Patron 178  Patron 179  Patron 180  Patron 181  Patron 182  Patron 183  Patron 184  Patron 185  Patron 186  Patron 187  Patron 188  Patron 189  Patron 190  Patron 191  Patron 192  Patron 193  Patron 194  Patron 195  Patron 196  Patron 197  Patron 198  Patron 199  Patron 200  Patron 201  Patron 202  Patron 203  Patron 204  Patron 205  Patron 206  Patron 207
          0 -0.000958 -0.001077  0.019883  0.019190 -0.003389 -0.014375 -0.005002 -0.004826 -0.018115 -0.000166  -0.017527   0.013798   0.008101   0.003558   0.008504  -0.017590  -0.013298  -0.011616  -0.001850   0.035764   0.051255   0.007508   0.063992  -0.005131  -0.000749  -0.021904  -0.012525  -0.023998  -0.036359   0.005511  -0.000698  -0.013897  -0.035243  -0.030307   0.009185  -0.013643  -0.012163  -0.041664  -0.038860  -0.007600   0.034325   0.012340   0.021467  -0.040926  -0.040892   0.004332   0.012684   0.003992  -0.022784   0.047793   0.074053   0.054379   0.042053   0.035975   0.098665   0.035483   0.029687   0.029064   0.045886   0.033593   0.091106   0.054667   0.077990   0.029712   0.024723   0.023231   0.025825   0.031538   0.039948   0.036617   0.016114   0.024884   0.049531   0.014503   0.014366   0.015395   0.065996   0.018323   0.040858   0.063346   0.108606   0.013939   0.016469   0.009037   0.029516   0.036750   0.073570   0.063267   0.071189   0.107914   0.053533   0.043966   0.044890   0.060258   0.012163   0.010760   0.025327   0.012752   0.048203   0.015770    0.123500    0.047134    0.004818    0.077277   -0.000205    0.000099   -0.004372    0.025426   -0.004794   -0.010249   -0.011381    0.025978    0.013268   -0.012161    0.023681   -0.013519   -0.006624   -0.018979   -0.025725   -0.003731    0.020852    0.024650    0.004509    0.045113   -0.018534   -0.013448    0.031141    0.035801    0.038923    0.001754   -0.021527    0.104645    0.006514    0.014699   -0.032829   -0.010828   -0.014864   -0.017208   -0.025338    0.002139   -0.013726   -0.009504   -0.030782   -0.015844    0.010121    0.018987    0.002886    0.063512    0.028044    0.018212    0.025781   -0.012633   -0.008027   -0.021846   -0.016320   -0.034785   -0.025189   -0.000020    0.008150    0.016417    0.007063   -0.002356   -0.005175    0.006579   -0.014602   -0.019155    0.051889    0.038794    0.078858    0.043002    0.029304    0.029901    0.044483    0.028723    0.060552    0.034003    0.034610    0.078896    0.030215    0.024300    0.035645    0.092306    0.037204    0.037991    0.040318    0.043138    0.068320    0.058941    0.064377    0.047676    0.080207    0.103200    0.103528    0.057036    0.052553    0.053390    0.016407    0.052613    0.069679    0.037678    0.017224    0.006073    0.049618    0.020704    0.036241    0.105793    0.033721    0.034069
          1 -0.000335 -0.000023  0.020473  0.021488 -0.002870 -0.008704 -0.001043 -0.000454 -0.013326  0.007903  -0.006839   0.018903   0.015635   0.011488   0.010286  -0.013477  -0.001891  -0.002254  -0.003489   0.037278   0.080342   0.034752   0.060259  -0.006573   0.006256  -0.016286  -0.001041  -0.016850  -0.021927   0.015499   0.013916  -0.001608  -0.024617  -0.017483   0.019905  -0.001844  -0.003412  -0.028280  -0.026253   0.001687   0.039248   0.023072   0.026341  -0.027403  -0.028572   0.013578   0.025107   0.014180  -0.012094   0.038770   0.065732   0.037478   0.031166   0.026268   0.079441   0.024986   0.024480   0.019784   0.032267   0.021010   0.080615   0.039040   0.070393   0.022774   0.018164   0.013855   0.016542   0.022938   0.028660   0.020147   0.008698   0.018505   0.043733   0.008851   0.017622   0.011631   0.045928   0.012460   0.027806   0.051157   0.090807   0.011475   0.008713   0.005992   0.023240   0.027098   0.057927   0.045152   0.051615   0.091515   0.040933   0.030947   0.029886   0.048332   0.006886   0.010828   0.016992   0.002299   0.037809   0.008590    0.099749    0.033120    0.000183    0.060613    0.004558    0.005033   -0.000178    0.024558   -0.001468   -0.006237   -0.006154    0.031148    0.015692   -0.005914    0.023437   -0.005443    0.000578   -0.007834   -0.011364   -0.002058    0.008442    0.016914    0.010804    0.041112   -0.011372    0.005055    0.036950    0.038597    0.042203    0.014590   -0.013336    0.116803    0.011763    0.017520   -0.017035   -0.002148   -0.002785   -0.008953   -0.017630    0.012751   -0.005316    0.007159   -0.017413   -0.004855    0.023355    0.029418    0.007570    0.066423    0.036770    0.022364    0.032794    0.002778   -0.001065   -0.011171   -0.004329   -0.022034   -0.014622    0.006307    0.015679    0.023914    0.019245    0.004516    0.003815    0.017103    0.000283   -0.010118    0.045219    0.027120    0.062600    0.029153    0.024037    0.019990    0.027486    0.018231    0.048315    0.026420    0.019052    0.061436    0.024342    0.021309    0.024535    0.076003    0.025640    0.025279    0.026609    0.030554    0.049751    0.044072    0.046356    0.034524    0.073146    0.088437    0.091790    0.050484    0.038007    0.038686    0.009442    0.039985    0.058878    0.033400    0.010761    0.006085    0.042319    0.016102    0.022187    0.096048    0.020806    0.025392
          2  0.003676  0.002593  0.019646  0.021323  0.001473 -0.001217  0.005020  0.005849 -0.005921  0.016384   0.000774   0.024003   0.022887   0.017094   0.015476  -0.006689   0.010453   0.009039  -0.000529   0.043650   0.093725   0.045553   0.065252  -0.004639   0.011004  -0.007862   0.006635  -0.009734  -0.013081   0.020302   0.026567   0.010631  -0.013449  -0.003235   0.025684   0.006311   0.006389  -0.018433  -0.018173   0.011751   0.053484   0.037603   0.032092  -0.016738  -0.020234   0.021245   0.034475   0.018007  -0.004736   0.028112   0.056834   0.026211   0.019869   0.018531   0.071373   0.017018   0.016875   0.013137   0.023235   0.011649   0.071076   0.028795   0.055780   0.014021   0.009375   0.007857   0.008322   0.013759   0.019668   0.010492   0.003531   0.009236   0.037297   0.003933   0.010590   0.004532   0.033332   0.005599   0.021229   0.042221   0.080991   0.002820   0.001632   0.001830   0.013810   0.019921   0.046968   0.034349   0.041570   0.079992   0.036139   0.025970   0.023335   0.044984   0.003211   0.004945   0.010726  -0.002511   0.032749   0.003911    0.088183    0.024347   -0.004027    0.048725    0.007278    0.007783    0.006224    0.027556    0.004428    0.000405   -0.000100    0.037356    0.023961    0.000107    0.031178    0.000301    0.006364   -0.001747   -0.002362    0.000877    0.006351    0.021079    0.023343    0.048680   -0.001771    0.020409    0.049521    0.045573    0.047244    0.028652   -0.003890    0.123308    0.017628    0.023923   -0.007672    0.007310    0.006010   -0.000161   -0.007470    0.023191   -0.001050    0.014962   -0.005003    0.007674    0.030560    0.035745    0.015749    0.077305    0.051050    0.036980    0.048230    0.013463    0.007220   -0.005058    0.003448   -0.015411   -0.006921    0.013048    0.022397    0.030064    0.027928    0.011428    0.006141    0.020482    0.005347   -0.004232    0.041909    0.018326    0.055066    0.016992    0.013742    0.012479    0.022484    0.012157    0.041032    0.021086    0.012898    0.048863    0.017990    0.013219    0.014636    0.068394    0.017673    0.017904    0.018289    0.020673    0.039297    0.034106    0.035488    0.023849    0.063528    0.078525    0.081789    0.041524    0.031373    0.031912    0.003804    0.033120    0.056074    0.032750    0.004412    0.002951    0.036288    0.013591    0.014379    0.084370    0.011890    0.015230
          3  0.001588  0.001573  0.012959  0.015718  0.000018 -0.002126  0.007158  0.004857 -0.004939  0.018902   0.003687   0.024314   0.024836   0.019910   0.016406  -0.005902   0.006452   0.016827   0.001271   0.043023   0.089659   0.039996   0.056469  -0.002780   0.007968  -0.007279   0.006351  -0.005797  -0.008482   0.019721   0.025186   0.012633  -0.008213   0.004268   0.027692   0.012859   0.012054  -0.013125  -0.012990   0.015631   0.059972   0.044904   0.034333  -0.008392  -0.013988   0.024170   0.039461   0.018568   0.000258   0.023514   0.050663   0.019572   0.015031   0.015289   0.070625   0.013684   0.011946   0.010093   0.019036   0.011529   0.067314   0.023673   0.045188   0.010097   0.009360   0.008445   0.005383   0.009103   0.018685   0.007079   0.004933   0.008982   0.036338   0.003794   0.011721   0.002972   0.024056   0.005090   0.017680   0.039525   0.075267   0.000265   0.000591   0.007371   0.010178   0.016846   0.040225   0.028680   0.036048   0.072664   0.034476   0.025310   0.022376   0.044421   0.003693   0.000889   0.009020  -0.001052   0.035161   0.004666    0.083679    0.018834   -0.003661    0.041570    0.006269    0.006765    0.004886    0.025400    0.004215    0.000395    0.000799    0.034820    0.026611    0.000818    0.033277    0.002802    0.009365   -0.000169   -0.000351   -0.000513    0.003188    0.016487    0.020632    0.040000   -0.001433    0.022457    0.048795    0.041895    0.040298    0.029202   -0.000944    0.117907    0.016522    0.027474   -0.005281    0.007771    0.008906    0.002698   -0.003615    0.025946   -0.002841    0.014538    0.002196    0.013611    0.031292    0.032010    0.019349    0.080464    0.059305    0.048099    0.057626    0.012240    0.014261   -0.000833    0.007903   -0.010382   -0.001238    0.017708    0.026522    0.033441    0.032444    0.016187    0.004733    0.020711    0.006497    0.000367    0.040595    0.013359    0.051529    0.013507    0.012483    0.009467    0.023253    0.008219    0.038284    0.017820    0.012120    0.040613    0.015640    0.009898    0.010111    0.062270    0.011966    0.012197    0.012008    0.011870    0.033116    0.028801    0.029085    0.017929    0.058250    0.071521    0.073996    0.037122    0.028451    0.030055    0.003127    0.028345    0.054950    0.033393    0.004856    0.003062    0.039337    0.019784    0.017075    0.072550    0.006174    0.008971
          4  0.000355 -0.000372  0.002175  0.005067 -0.001699 -0.003055  0.008094 -0.001458 -0.005759  0.017762   0.000667   0.021407   0.022064   0.023535   0.014497  -0.006582   0.001308   0.015919  -0.000038   0.038291   0.077343   0.028847   0.046775  -0.000992   0.006015  -0.005748   0.001106  -0.005410  -0.007429   0.013910   0.013078   0.007661  -0.007271   0.008976   0.026362   0.015431   0.018035  -0.009778  -0.010249   0.010757   0.053376   0.042699   0.029316  -0.005595  -0.010134   0.022937   0.040895   0.011426   0.000699   0.024440   0.043299   0.014700   0.010831   0.011219   0.069133   0.011408   0.008799   0.008946   0.019233   0.014316   0.071815   0.027358   0.029756   0.008771   0.008724   0.007596   0.007763   0.008871   0.023336   0.008743   0.006730   0.009267   0.037942   0.006584   0.009835   0.008742   0.021263   0.009691   0.018796   0.039050   0.067452   0.004740   0.004777   0.014873   0.005673   0.017896   0.035768   0.023065   0.029942   0.073906   0.034335   0.027338   0.019798   0.039994   0.005689   0.004546   0.005697   0.003367   0.039712   0.008317    0.077633    0.016804    0.003037    0.036792    0.000426    0.001188   -0.000300    0.020365   -0.000712   -0.001814   -0.002010    0.029582    0.024699   -0.002240    0.030047    0.003399    0.009631   -0.001854    0.003985   -0.001908   -0.002019    0.004378    0.015414    0.028112   -0.001236    0.031303    0.054865    0.025026    0.036490    0.026444   -0.001454    0.099426    0.011093    0.022392   -0.003747    0.004275    0.006919    0.002913   -0.003513    0.024605   -0.004649    0.008405    0.005146    0.013444    0.029857    0.022792    0.020878    0.080360    0.060307    0.044002    0.053279    0.002122    0.014537    0.000160    0.008718   -0.006138    0.001907    0.021285    0.027459    0.032911    0.033179    0.018697   -0.000892    0.013108   -0.000079    0.003723    0.036654    0.008668    0.046767    0.009602    0.022411    0.008235    0.032008    0.006890    0.040302    0.019162    0.015173    0.033296    0.022902    0.007291    0.006869    0.053968    0.009888    0.007665    0.009786    0.006288    0.027289    0.023522    0.023906    0.016563    0.056136    0.074506    0.076444    0.035124    0.027842    0.032263    0.003984    0.022897    0.048478    0.029927    0.004251    0.003839    0.040081    0.026158    0.022636    0.054386    0.004084    0.008015
          5  0.002126  0.001616  0.003389  0.002866  0.001408  0.001095  0.008433  0.001552  0.000109  0.022066   0.008315   0.024636   0.025359   0.032492   0.022072   0.000023   0.000758   0.019274   0.005505   0.039032   0.078884   0.027238   0.034212   0.006314   0.008052  -0.000399   0.000484  -0.000272  -0.001013   0.016280   0.012724   0.003448  -0.001271   0.020632   0.032865   0.024902   0.029822  -0.001555  -0.001538   0.016899   0.060801   0.048168   0.031073   0.002345  -0.001710   0.027793   0.047712   0.011585   0.003978   0.015999   0.029020   0.004858   0.003797   0.003659   0.058500   0.003745   0.002927   0.002522   0.013710   0.007277   0.061351   0.027378   0.021390   0.002396   0.002090   0.002507   0.002094   0.006456   0.020518   0.008282   0.001713   0.008492   0.036602   0.001969   0.010877   0.010465   0.006072   0.002217   0.009374   0.032020   0.060007   0.001864   0.001356   0.010125   0.001337   0.013021   0.025710   0.010156   0.021006   0.066772   0.028124   0.022665   0.013472   0.036130   0.002317   0.001121   0.002744   0.000585   0.036797   0.002952    0.062556    0.011802   -0.000093    0.029784    0.001784    0.002110    0.001703    0.022290    0.001835    0.001077    0.000799    0.029740    0.031388    0.000966    0.030480    0.007336    0.015746    0.003504    0.013884    0.007214    0.000906    0.008323    0.017099    0.021411    0.001397    0.039768    0.063844    0.029423    0.031274    0.041299    0.002191    0.093928    0.016772    0.026204    0.000220    0.007709    0.012307    0.006647    0.001042    0.028119    0.000762    0.010469    0.014208    0.021775    0.035752    0.028828    0.028781    0.085391    0.064930    0.050750    0.057782    0.001015    0.020822    0.004966    0.016021   -0.000431    0.005029    0.025304    0.033241    0.040503    0.040201    0.023096    0.000286    0.016087    0.001321    0.008354    0.026850    0.003366    0.045691    0.002352    0.032755    0.002290    0.032400    0.002099    0.030032    0.008847    0.007937    0.023292    0.019846    0.002621    0.002049    0.043822    0.003807    0.003019    0.006371    0.003177    0.015834    0.013415    0.013470    0.010093    0.047933    0.064025    0.065377    0.031314    0.020819    0.027112    0.001613    0.015186    0.042350    0.026731    0.001845    0.001432    0.035324    0.026114    0.014518    0.030109    0.000309    0.001299
          6  0.001905  0.002388  0.002630  0.002366  0.000333  0.000195  0.008028  0.000949  0.000274  0.022071   0.009428   0.025374   0.025777   0.033339   0.023124   0.000829   0.001770   0.019976   0.007578   0.040028   0.079030   0.027186   0.033916   0.007992   0.008297   0.000121   0.000111   0.002072   0.001426   0.017375   0.012535   0.005592   0.000800   0.021292   0.033525   0.025207   0.032329   0.000264   0.000172   0.018680   0.063805   0.050592   0.032454   0.004028   0.000056   0.029557   0.048944   0.013178   0.005777   0.016584   0.028329   0.002485   0.001866   0.001714   0.055168   0.002611   0.002311   0.002012   0.012173   0.006476   0.061456   0.026938   0.021131   0.002136   0.002261   0.002450   0.001745   0.005490   0.021077   0.007499   0.001832   0.007515   0.035525   0.001948   0.011850   0.011573   0.004982   0.001567   0.008500   0.030922   0.058139   0.000682   0.000967   0.009874   0.001535   0.011648   0.024463   0.008488   0.019416   0.066227   0.027173   0.021869   0.012675   0.035170   0.001953   0.001096   0.001174   0.000321   0.035346   0.002485    0.061889    0.011628    0.000053    0.028394    0.001972    0.002655    0.001456    0.022120    0.001211    0.000953    0.000703    0.028859    0.031411    0.000766    0.030753    0.008724    0.016981    0.004736    0.014998    0.007844    0.000519    0.008107    0.018790    0.023329    0.001689    0.038672    0.065671    0.032229    0.033010    0.041970    0.002680    0.093492    0.017324    0.026912    0.000363    0.007345    0.014629    0.008854    0.003718    0.030967    0.000926    0.009650    0.015363    0.022505    0.037525    0.031328    0.030874    0.087238    0.067273    0.052441    0.060088    0.003931    0.022054    0.006553    0.017597    0.001003    0.006379    0.026731    0.034755    0.042118    0.041447    0.024382    0.002540    0.017417    0.003317    0.010878    0.025374    0.002368    0.044651    0.001897    0.032726    0.002552    0.031903    0.001290    0.029027    0.008770    0.006534    0.022436    0.019648    0.002828    0.000741    0.042841    0.003141    0.001750    0.005069    0.001618    0.014236    0.012210    0.012151    0.008314    0.047084    0.062982    0.064748    0.028931    0.020408    0.026486    0.000106    0.014462    0.040810    0.026613    0.000400    0.000552    0.033547    0.024521    0.011995    0.030134    0.000587    0.000135

[OK] Batch 1 sauvegard├® avec succ├¿s dans le fichier : **Q5\Q5_Stabilites des perceptrons entrain├®s sur l'ensemble complet.png**

================================================================================
R├ëSUM├ë DE L'EXPORTATION
================================================================================
Total de lignes trait├®es : 7
Taille du batch : 100
Nombre de batches g├®n├®r├®s : 1
Iterations 10000 : Erreur = 65/208 = 31.25%
Iterations 20000 : Erreur = 52/208 = 25.00%
Iterations 30000 : Erreur = 31/208 = 14.90%
Iterations 50000 : Erreur = 25/208 = 12.02%
Iterations 100000 : Erreur = 32/208 = 15.38%
Iterations 200000 : Erreur = 9/208 = 4.33%
Iterations 500000 : Erreur = 0/208 = 0.00%
-------------------------------------------------- 

CONCLUSION
FAUX !! L'algortihme ne converge jamais : on en conclut que L n'est pas lin├®airement s├®parable. En effet, P = 208 > 2N = 120=> l'existence d'un hyperplan s├®parateur n'est pas garantie.

================================================================================
FIN DU TP2
================================================================================
